#!/usr/bin/env python3
"""
Atomic Model Persistence System
Granular saving and loading of RL models per instrument, timeframe, and agent

Features:
- Atomic saves prevent corruption during system interruption
- Versioned model states with rollback capability
- Performance metrics tracking per granular level
- Delta compression for efficient storage
- Concurrent access protection
"""

import os
import json
import pickle
import threading
import hashlib
import gzip
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import logging

@dataclass
class ModelSnapshot:
    """Complete model state snapshot"""
    agent_name: str
    symbol: str 
    timeframe: str
    version: int
    model_weights: bytes  # Compressed model weights
    experience_buffer: List[Dict]  # Recent experiences
    performance_metrics: Dict[str, float]
    regime_adaptations: Dict[str, Any]
    learning_history: List[Dict]
    metadata: Dict[str, Any]
    checksum: str
    timestamp: datetime
    file_size_bytes: int

@dataclass
class AtomicSaveConfig:
    """Configuration for atomic saving system"""
    base_directory: str = "./rl_models"
    max_versions_per_model: int = 10
    save_interval_seconds: int = 30
    compression_level: int = 6
    verify_checksums: bool = True
    enable_delta_compression: bool = True
    max_concurrent_saves: int = 4

class AtomicModelPersistence:
    """Atomic persistence system for RL models"""
    
    def __init__(self, config: Optional[AtomicSaveConfig] = None):
        self.config = config or AtomicSaveConfig()
        self.base_path = Path(self.config.base_directory)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # Thread safety
        self.save_lock = threading.RLock()
        self.save_semaphore = threading.Semaphore(self.config.max_concurrent_saves)
        
        # Model registry
        self.model_registry: Dict[str, ModelSnapshot] = {}
        self.save_queue: List[Tuple[str, Dict]] = []
        
        # Background worker
        self.save_worker_thread = None
        self.is_running = False
        
        # Performance tracking
        self.save_stats = {
            'total_saves': 0,
            'successful_saves': 0,
            'failed_saves': 0,
            'total_bytes_saved': 0,
            'avg_save_time_ms': 0
        }
        
        print("üíæ Atomic Model Persistence System initialized")
        print(f"   üìÅ Base directory: {self.base_path}")
        print(f"   üîÑ Max versions per model: {self.config.max_versions_per_model}")
        print(f"   ‚è∞ Save interval: {self.config.save_interval_seconds}s")
    
    def start_persistence_worker(self):
        """Start background persistence worker"""
        
        if self.is_running:
            return
            
        self.is_running = True
        self.save_worker_thread = threading.Thread(
            target=self._persistence_worker,
            daemon=True,
            name="AtomicPersistenceWorker"
        )
        self.save_worker_thread.start()
        
        print("üöÄ Atomic persistence worker started")
    
    def stop_persistence_worker(self):
        """Stop background persistence worker"""
        
        self.is_running = False
        if self.save_worker_thread and self.save_worker_thread.is_alive():
            self.save_worker_thread.join(timeout=30)
        
        print("‚èπÔ∏è Atomic persistence worker stopped")
    
    def _persistence_worker(self):
        """Background worker for handling saves"""
        
        import time
        
        while self.is_running:
            try:
                # Process save queue
                if self.save_queue:
                    with self.save_lock:
                        pending_saves = self.save_queue.copy()
                        self.save_queue.clear()
                    
                    for model_key, model_data in pending_saves:
                        self._perform_atomic_save(model_key, model_data)
                
                time.sleep(self.config.save_interval_seconds)
                
            except Exception as e:
                print(f"‚ùå Persistence worker error: {e}")
                time.sleep(60)  # Wait before retrying
    
    def queue_model_save(self, agent_name: str, symbol: str, timeframe: str, 
                        model_data: Dict[str, Any]):
        """Queue a model for atomic saving"""
        
        model_key = self._get_model_key(agent_name, symbol, timeframe)
        
        with self.save_lock:
            # Remove any existing queued save for this model
            self.save_queue = [(k, d) for k, d in self.save_queue if k != model_key]
            # Add new save request
            self.save_queue.append((model_key, model_data))
        
        print(f"üìù Queued save: {model_key}")
    
    def _perform_atomic_save(self, model_key: str, model_data: Dict[str, Any]):
        """Perform atomic save with transactional guarantees"""
        
        import time
        
        save_start = time.time()
        
        # Acquire semaphore to limit concurrent saves
        with self.save_semaphore:
            try:
                parts = model_key.split('_')
                agent_name, symbol, timeframe = parts[0], parts[1], parts[2]
                
                # Create model directory
                model_dir = self.base_path / agent_name / symbol / timeframe
                model_dir.mkdir(parents=True, exist_ok=True)
                
                # Determine next version number
                version = self._get_next_version(model_dir)
                
                # Prepare snapshot
                snapshot = self._create_model_snapshot(
                    agent_name, symbol, timeframe, version, model_data
                )
                
                # Atomic save process
                temp_file = model_dir / f"v{version}.tmp"
                final_file = model_dir / f"v{version}.model"
                
                # Save to temporary file first
                self._save_snapshot_to_file(snapshot, temp_file)
                
                # Verify the save
                if self.config.verify_checksums:
                    if not self._verify_snapshot_integrity(temp_file, snapshot.checksum):
                        raise Exception("Checksum verification failed")
                
                # Atomic move to final location
                shutil.move(str(temp_file), str(final_file))
                
                # Update registry
                self.model_registry[model_key] = snapshot
                
                # Cleanup old versions
                self._cleanup_old_versions(model_dir)
                
                # Update statistics
                save_time = (time.time() - save_start) * 1000
                self._update_save_stats(True, snapshot.file_size_bytes, save_time)
                
                print(f"üíæ Atomic save completed: {model_key} v{version} ({snapshot.file_size_bytes} bytes)")
                
            except Exception as e:
                self._update_save_stats(False, 0, 0)
                print(f"‚ùå Atomic save failed for {model_key}: {e}")
                
                # Cleanup temp file if it exists
                temp_file_path = self.base_path / agent_name / symbol / timeframe / f"v{version}.tmp"
                if temp_file_path.exists():
                    temp_file_path.unlink()
    
    def _create_model_snapshot(self, agent_name: str, symbol: str, timeframe: str,
                              version: int, model_data: Dict[str, Any]) -> ModelSnapshot:
        """Create a complete model snapshot"""
        
        # Compress model weights if available
        model_weights = model_data.get('model_weights', b'')
        if isinstance(model_weights, str):
            model_weights = model_weights.encode('utf-8')
        
        compressed_weights = gzip.compress(model_weights, compresslevel=self.config.compression_level)
        
        # Extract other components
        experience_buffer = model_data.get('experience_buffer', [])
        performance_metrics = model_data.get('performance_metrics', {})
        regime_adaptations = model_data.get('regime_adaptations', {})
        learning_history = model_data.get('learning_history', [])
        
        # Create snapshot data
        snapshot_data = {
            'agent_name': agent_name,
            'symbol': symbol,
            'timeframe': timeframe,
            'version': version,
            'model_weights': compressed_weights,
            'experience_buffer': experience_buffer,
            'performance_metrics': performance_metrics,
            'regime_adaptations': regime_adaptations,
            'learning_history': learning_history,
            'metadata': {
                'model_type': model_data.get('model_type', 'RL'),
                'training_steps': model_data.get('training_steps', 0),
                'creation_time': datetime.now().isoformat(),
                'compression_ratio': len(model_weights) / len(compressed_weights) if model_weights else 1.0
            },
            'timestamp': datetime.now()
        }
        
        # Calculate checksum
        checksum = self._calculate_checksum(snapshot_data)
        
        snapshot = ModelSnapshot(
            agent_name=agent_name,
            symbol=symbol,
            timeframe=timeframe,
            version=version,
            model_weights=compressed_weights,
            experience_buffer=experience_buffer,
            performance_metrics=performance_metrics,
            regime_adaptations=regime_adaptations,
            learning_history=learning_history,
            metadata=snapshot_data['metadata'],
            checksum=checksum,
            timestamp=snapshot_data['timestamp'],
            file_size_bytes=0  # Will be set after saving
        )
        
        return snapshot
    
    def _save_snapshot_to_file(self, snapshot: ModelSnapshot, file_path: Path):
        """Save snapshot to file with proper serialization"""
        
        # Convert to dictionary for serialization
        snapshot_dict = asdict(snapshot)
        
        # Handle bytes serialization
        snapshot_dict['model_weights'] = snapshot.model_weights
        
        try:
            with open(file_path, 'wb') as f:
                pickle.dump(snapshot_dict, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # Update file size
            snapshot.file_size_bytes = file_path.stat().st_size
            
        except Exception as e:
            if file_path.exists():
                file_path.unlink()
            raise e
    
    def load_model_snapshot(self, agent_name: str, symbol: str, timeframe: str, 
                           version: Optional[int] = None) -> Optional[ModelSnapshot]:
        """Load model snapshot with atomic guarantees"""
        
        model_key = self._get_model_key(agent_name, symbol, timeframe)
        
        # Check registry first
        if model_key in self.model_registry and version is None:
            return self.model_registry[model_key]
        
        # Load from disk
        model_dir = self.base_path / agent_name / symbol / timeframe
        if not model_dir.exists():
            return None
        
        # Find target version
        if version is None:
            version = self._get_latest_version(model_dir)
            if version is None:
                return None
        
        model_file = model_dir / f"v{version}.model"
        if not model_file.exists():
            return None
        
        try:
            with open(model_file, 'rb') as f:
                snapshot_dict = pickle.load(f)
            
            # Convert back to ModelSnapshot
            snapshot = ModelSnapshot(**snapshot_dict)
            
            # Verify integrity if enabled
            if self.config.verify_checksums:
                if not self._verify_loaded_snapshot(snapshot):
                    print(f"‚ö†Ô∏è Checksum verification failed for {model_key} v{version}")
                    return None
            
            # Update registry
            self.model_registry[model_key] = snapshot
            
            print(f"üìÅ Loaded model: {model_key} v{version}")
            return snapshot
            
        except Exception as e:
            print(f"‚ùå Failed to load {model_key} v{version}: {e}")
            return None
    
    def list_model_versions(self, agent_name: str, symbol: str, timeframe: str) -> List[Tuple[int, datetime, int]]:
        """List all available versions for a model"""
        
        model_dir = self.base_path / agent_name / symbol / timeframe
        if not model_dir.exists():
            return []
        
        versions = []
        for model_file in model_dir.glob("v*.model"):
            try:
                version = int(model_file.stem[1:])  # Remove 'v' prefix
                stat = model_file.stat()
                timestamp = datetime.fromtimestamp(stat.st_mtime)
                size = stat.st_size
                versions.append((version, timestamp, size))
            except (ValueError, OSError):
                continue
        
        return sorted(versions, key=lambda x: x[0], reverse=True)
    
    def delete_model_versions(self, agent_name: str, symbol: str, timeframe: str, 
                            versions_to_keep: int = 3) -> int:
        """Delete old model versions, keeping specified number"""
        
        model_dir = self.base_path / agent_name / symbol / timeframe
        if not model_dir.exists():
            return 0
        
        versions = self.list_model_versions(agent_name, symbol, timeframe)
        if len(versions) <= versions_to_keep:
            return 0
        
        deleted_count = 0
        versions_to_delete = versions[versions_to_keep:]
        
        for version, _, _ in versions_to_delete:
            model_file = model_dir / f"v{version}.model"
            try:
                model_file.unlink()
                deleted_count += 1
            except OSError:
                pass
        
        print(f"üóëÔ∏è Deleted {deleted_count} old versions for {agent_name}_{symbol}_{timeframe}")
        return deleted_count
    
    def get_model_statistics(self) -> Dict[str, Any]:
        """Get comprehensive statistics about saved models"""
        
        total_models = 0
        total_versions = 0
        total_size_bytes = 0
        agents = set()
        symbols = set()
        timeframes = set()
        
        for agent_dir in self.base_path.iterdir():
            if not agent_dir.is_dir():
                continue
            agents.add(agent_dir.name)
            
            for symbol_dir in agent_dir.iterdir():
                if not symbol_dir.is_dir():
                    continue
                symbols.add(symbol_dir.name)
                
                for timeframe_dir in symbol_dir.iterdir():
                    if not timeframe_dir.is_dir():
                        continue
                    timeframes.add(timeframe_dir.name)
                    
                    total_models += 1
                    
                    # Count versions and sizes
                    for model_file in timeframe_dir.glob("v*.model"):
                        total_versions += 1
                        try:
                            total_size_bytes += model_file.stat().st_size
                        except OSError:
                            pass
        
        return {
            'total_models': total_models,
            'total_versions': total_versions,
            'total_size_mb': total_size_bytes / (1024 * 1024),
            'unique_agents': len(agents),
            'unique_symbols': len(symbols),
            'unique_timeframes': len(timeframes),
            'save_statistics': self.save_stats,
            'agents': sorted(agents),
            'symbols': sorted(symbols),
            'timeframes': sorted(timeframes)
        }
    
    def _get_model_key(self, agent_name: str, symbol: str, timeframe: str) -> str:
        """Generate unique key for model"""
        return f"{agent_name}_{symbol}_{timeframe}"
    
    def _get_next_version(self, model_dir: Path) -> int:
        """Get next version number for model"""
        
        versions = []
        for model_file in model_dir.glob("v*.model"):
            try:
                version = int(model_file.stem[1:])  # Remove 'v' prefix
                versions.append(version)
            except ValueError:
                continue
        
        return max(versions) + 1 if versions else 1
    
    def _get_latest_version(self, model_dir: Path) -> Optional[int]:
        """Get latest version number for model"""
        
        versions = []
        for model_file in model_dir.glob("v*.model"):
            try:
                version = int(model_file.stem[1:])
                versions.append(version)
            except ValueError:
                continue
        
        return max(versions) if versions else None
    
    def _cleanup_old_versions(self, model_dir: Path):
        """Clean up old versions beyond max limit"""
        
        versions = []
        for model_file in model_dir.glob("v*.model"):
            try:
                version = int(model_file.stem[1:])
                versions.append((version, model_file))
            except ValueError:
                continue
        
        # Sort by version number, newest first
        versions.sort(key=lambda x: x[0], reverse=True)
        
        # Delete excess versions
        if len(versions) > self.config.max_versions_per_model:
            for version, model_file in versions[self.config.max_versions_per_model:]:
                try:
                    model_file.unlink()
                except OSError:
                    pass
    
    def _calculate_checksum(self, data: Dict) -> str:
        """Calculate checksum for data integrity"""
        
        # Create a deterministic string representation
        data_str = json.dumps(data, sort_keys=True, default=str).encode('utf-8')
        return hashlib.sha256(data_str).hexdigest()
    
    def _verify_snapshot_integrity(self, file_path: Path, expected_checksum: str) -> bool:
        """Verify snapshot file integrity"""
        
        try:
            with open(file_path, 'rb') as f:
                snapshot_dict = pickle.load(f)
            
            calculated_checksum = self._calculate_checksum(snapshot_dict)
            return calculated_checksum == expected_checksum
            
        except Exception:
            return False
    
    def _verify_loaded_snapshot(self, snapshot: ModelSnapshot) -> bool:
        """Verify loaded snapshot integrity"""
        
        # Reconstruct data for checksum calculation
        data = {
            'agent_name': snapshot.agent_name,
            'symbol': snapshot.symbol,
            'timeframe': snapshot.timeframe,
            'version': snapshot.version,
            'model_weights': snapshot.model_weights,
            'experience_buffer': snapshot.experience_buffer,
            'performance_metrics': snapshot.performance_metrics,
            'regime_adaptations': snapshot.regime_adaptations,
            'learning_history': snapshot.learning_history,
            'metadata': snapshot.metadata,
            'timestamp': snapshot.timestamp
        }
        
        calculated_checksum = self._calculate_checksum(data)
        return calculated_checksum == snapshot.checksum
    
    def _update_save_stats(self, success: bool, bytes_saved: int, save_time_ms: float):
        """Update save statistics"""
        
        self.save_stats['total_saves'] += 1
        if success:
            self.save_stats['successful_saves'] += 1
            self.save_stats['total_bytes_saved'] += bytes_saved
            
            # Update average save time
            total_successful = self.save_stats['successful_saves']
            current_avg = self.save_stats['avg_save_time_ms']
            self.save_stats['avg_save_time_ms'] = ((current_avg * (total_successful - 1)) + save_time_ms) / total_successful
        else:
            self.save_stats['failed_saves'] += 1

def demonstrate_atomic_persistence():
    """Demonstrate the atomic persistence system"""
    
    print("üíæ ATOMIC MODEL PERSISTENCE DEMONSTRATION")
    print("=" * 70)
    
    # Create persistence system
    config = AtomicSaveConfig(
        base_directory="./demo_models",
        max_versions_per_model=5,
        save_interval_seconds=5
    )
    
    persistence = AtomicModelPersistence(config)
    persistence.start_persistence_worker()
    
    try:
        # Simulate model training and saving
        agents = ['BERSERKER', 'SNIPER']
        symbols = ['EURUSD+', 'GBPUSD+', 'BTCUSD+']
        timeframes = ['M5', 'M15', 'H1']
        
        print("üîÑ Simulating model training and atomic saving...")
        
        for agent in agents:
            for symbol in symbols:
                for timeframe in timeframes:
                    # Simulate model data
                    model_data = {
                        'model_weights': f"Mock weights for {agent}_{symbol}_{timeframe}",
                        'experience_buffer': [
                            {'state': 'mock_state', 'action': 'BUY', 'reward': 0.5},
                            {'state': 'mock_state2', 'action': 'SELL', 'reward': -0.2}
                        ],
                        'performance_metrics': {
                            'win_rate': 0.65,
                            'profit_factor': 1.8,
                            'max_drawdown': 0.12
                        },
                        'regime_adaptations': {
                            'CHAOTIC': 0.9 if agent == 'BERSERKER' else 0.3,
                            'UNDERDAMPED': 0.7,
                            'CRITICALLY_DAMPED': 0.4 if agent == 'BERSERKER' else 0.8,
                            'OVERDAMPED': 0.2 if agent == 'BERSERKER' else 0.7
                        },
                        'learning_history': [
                            {'step': 1000, 'loss': 0.5},
                            {'step': 2000, 'loss': 0.3}
                        ],
                        'model_type': f'{agent}_RL_Model',
                        'training_steps': 5000
                    }
                    
                    # Queue for atomic save
                    persistence.queue_model_save(agent, symbol, timeframe, model_data)
        
        # Wait for saves to complete
        import time
        time.sleep(15)
        
        # Demonstrate loading
        print("\nüìÅ Testing model loading...")
        snapshot = persistence.load_model_snapshot('BERSERKER', 'EURUSD+', 'M15')
        if snapshot:
            print(f"‚úÖ Loaded BERSERKER EURUSD+ M15 v{snapshot.version}")
            print(f"   Performance: {snapshot.performance_metrics}")
        
        # Show statistics
        stats = persistence.get_model_statistics()
        print(f"\nüìä PERSISTENCE STATISTICS:")
        print(f"   Total models: {stats['total_models']}")
        print(f"   Total versions: {stats['total_versions']}")
        print(f"   Storage used: {stats['total_size_mb']:.2f} MB")
        print(f"   Successful saves: {stats['save_statistics']['successful_saves']}")
        
        # List versions
        versions = persistence.list_model_versions('SNIPER', 'BTCUSD+', 'H1')
        print(f"\nüìã SNIPER BTCUSD+ H1 versions: {len(versions)}")
        for version, timestamp, size in versions[:3]:
            print(f"   v{version}: {timestamp.strftime('%Y-%m-%d %H:%M:%S')} ({size} bytes)")
        
    finally:
        persistence.stop_persistence_worker()
        print("‚úÖ Atomic persistence demonstration completed")

if __name__ == "__main__":
    demonstrate_atomic_persistence()